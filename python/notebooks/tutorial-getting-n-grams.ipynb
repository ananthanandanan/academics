{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8741067d5d269c94ac1ae91e19184cd608123bbf"
   },
   "source": [
    "### The order words are used in is important\n",
    "\n",
    "The order that words are used in text is not random. In English, for example, you can say \"the red apple\" but not \"apple red the\". The relationships between words in text is very complex. So complex, in fact, that there's an entire field of linguistics devoted to it: syntax. (You can learn more about syntax [here](https://pdfs.semanticscholar.org/ba33/a9656f43a6b7420a180bdeccd5be98fc05eb.pdf), if you're interested.)\n",
    "\n",
    "However, there is a relatively simply way of capturing some of these relationships between words, originally proposed by Warren Weaver. You can capture quite a bit of information by just looking at which words tend to show up next to each other more often. \n",
    "\n",
    "\n",
    "### What are ngrams? \n",
    "\n",
    "The general idea is that you can look at each pair (or triple, set of four, etc.) of words that occur next to each other. In a sufficently-large corpus, you're likely to see \"the red\" and \"red apple\" several times, but less likely to see \"apple red\" and \"red the\". This is useful to know if, for example, you're trying to figure out what someone is more likely to say to help decide between possible output for an automatic speech recognition system.\n",
    "\n",
    "These co-occuring words are known as \"n-grams\", where \"n\" is a number saying how long a string of words you considered. (Unigrams are single words, bigrams are two words, trigrams are three words, 4-grams are four words, 5-grams are five words, etc.)\n",
    "\n",
    "\n",
    "> **Learning goal:** In this tutorial, you'll learn how to find all the n-grams in a corpus & count how often each is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "99e0adf8-2672-438d-9641-66c2fe4651b2",
    "_uuid": "394b7ec02251c51466dba97e843c0d2fd90cd043"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<doc id=\"32449\" title=\"Himno de las Américas\" nonfiltered=\"1\" processed=\"1\" dbindex=\"15001\">\\nEl Himno de las Américas es cantado en todas las ceremonias y actos oficiales en conmemoración al Día de las Américas (14 de abril). Letra y música de Rodolfo Sciamarella.\\n\\n Letra .\\n\\nUn canto de amistad, de buena vecindad, ;\\nunidos nos tendrá eternamente. ;\\nPor nuestra libertad, por nuestra lealtad, ;\\ntenemos que vivir gloriosamente.\\n\\nUn símbolo de paz alumbrará el vivir ;\\nde todo el Continente Americano. ;\\nFuerza de Optimismo, fuerza de Hermandad ;\\nserá este canto de buena vecindad.\\n\\nArgentina, Brasil y Bolivia,\\nColombia, Chile y Ecuador;\\nUruguay, Venezuela y Honduras ;\\nGuatemala y El Salvador,\\nCosta Rica, Haití, Nicaragua,\\nCuba y Paraguay, ;\\nNorteamérica, México y Perú,\\nSanto Domingo y Panamá;\\n\\n¡Son hermanos soberanos de la libertad!\\n¡Son hermanos soberanos de la libertad!\\n\\nEn otras versiones se incluyen Cuba y Canadá.\\nSanto Domingo es el nombre colonial de República Dominicana\\n\\n Enlaces exte'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in all the modules we're going to need\n",
    "import nltk, re, string, collections\n",
    "from nltk.util import ngrams # function for making ngrams\n",
    "\n",
    "# this corpus is pretty big, so let's look at just one of the files in it\n",
    "with open(\"../input/spanish_corpus/spanishText_15000_20000\", \"r\", encoding='latin-1') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# check to make sure the file read in alright; let's print out the first 1000 characters\n",
    "text[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "430143b5f28c56d6fb20747d4e4b1bf7d14f6eb7"
   },
   "source": [
    "Looking at the text, you can see that there's some extra xml markup that we don't really want to analyze (the suff in <pointy brackets>). Let's get rid of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "60a4f934a550fe8d033168a96e2337f3ceca4eaa",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEl Himno de las Américas es cantado en todas las ceremonias y actos oficiales en conmemoración al Día de las Américas 14 de abril Letra y música de Rodolfo Sciamarella\\n\\n Letra \\n\\nUn canto de amistad de buena vecindad \\nunidos nos tendrá eternamente \\nPor nuestra libertad por nuestra lealtad \\ntenemos que vivir gloriosamente\\n\\nUn símbolo de paz alumbrará el vivir \\nde todo el Continente Americano \\nFuerza de Optimismo fuerza de Hermandad \\nserá este canto de buena vecindad\\n\\nArgentina Brasil y Bolivia\\nColombia Chile y Ecuador\\nUruguay Venezuela y Honduras \\nGuatemala y El Salvador\\nCosta Rica Haití Nicaragua\\nCuba y Paraguay \\nNorteamérica México y Perú\\nSanto Domingo y Panamá\\n\\n¡Son hermanos soberanos de la libertad\\n¡Son hermanos soberanos de la libertad\\n\\nEn otras versiones se incluyen Cuba y Canadá\\nSanto Domingo es el nombre colonial de República Dominicana\\n\\n Enlaces externos \\n Canción en YouTube\\n\\n\\n\\n\\n\\n\\n\\n\\nEl King Kong es un dulce típico de la Gastronomía del Perú originario de la zona norte específic'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's do some preprocessing. We don't care about the XML notation, new lines \n",
    "# or punctuation marks other than periods. (We'll consider the end of the sentence\n",
    "# a \"word\") We also don't want to consider capitalization. \n",
    "\n",
    "# get rid of all the XML markup\n",
    "text = re.sub('<.*>','',text)\n",
    "\n",
    "# get rid of the \"ENDOFARTICLE.\" text\n",
    "text = re.sub('ENDOFARTICLE.','',text)\n",
    "\n",
    "# get rid of punctuation (except periods!)\n",
    "punctuationNoPeriod = \"[\" + re.sub(\"\\.\",\"\",string.punctuation) + \"]\"\n",
    "text = re.sub(punctuationNoPeriod, \"\", text)\n",
    "\n",
    "# make sure it looks ok\n",
    "text[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "28d2b6735cde4eb44e95a47aea2016228e02e5f8"
   },
   "source": [
    "Ok, now onto making the n-grams! The first thing we want to do is \"tokenize\", or break the  text into indvidual words (you can find more information on tokenization [here](https://www.kaggle.com/rtatman/tokenization-tutorial))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "92145cdfb644377865c25c943f07449c16792cfe",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# first get individual words\n",
    "tokenized = text.split()\n",
    "\n",
    "# and get a list of all the bi-grams\n",
    "esBigrams = ngrams(tokenized, 2)\n",
    "\n",
    "# If you like, you can uncomment the next like to take a look at \n",
    "# the first ten to make sure they look ok. Please note that doing so \n",
    "# will consume the generator & will break the next block of code, so you'll\n",
    "# need to re-comment it and run this block again to get it to work.\n",
    "# list(esBigrams)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a4990722239e09596e6eb63223bcd0d06cb284d9"
   },
   "source": [
    "Yay, we've got our n-grams! Just a list of all the bigrams in a text isn't that useful, though. Let's collapse it a little bit & get a count of how frequently we see each bigram in our corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "a998670747c5c38314c95d517b3896122e999485",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('de', 'la'), 45682),\n",
       " (('de', 'los'), 19374),\n",
       " (('en', 'el'), 19323),\n",
       " (('en', 'la'), 18617),\n",
       " (('a', 'la'), 12692),\n",
       " (('de', 'las'), 11335),\n",
       " (('que', 'se'), 8181),\n",
       " (('y', 'el'), 6498),\n",
       " (('y', 'la'), 6286),\n",
       " (('a', 'los'), 5330)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the frequency of each bigram in our corpus\n",
    "esBigramFreq = collections.Counter(esBigrams)\n",
    "\n",
    "# what are the ten most popular ngrams in this Spanish corpus?\n",
    "esBigramFreq.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4133a80b9229a4f829b5174f91492bb63f64f64c"
   },
   "source": [
    "I'm not a fluent Spanish speaker, but those do look like some reasonable very frequent two-word phrases in Spanish.\n",
    "\n",
    "And that's all there is to it! Here are some exercises to help you try it out yourself.\n",
    "\n",
    "\n",
    "### Your turn!\n",
    "\n",
    "1. Find the most frequent n-grams in another file in this corpus. You can find the other files by entering \"../input/spanish_corpus/\" in a code chunk and then hitting the Tab key. All of the files will be listed in a drop-down menu. Are the most frequent bigrams the same as they were in this file?\n",
    "2. Instead of bigrams (two word phrases), can you find trigrams (three words)?\n",
    "3. Find the most frequent ngrams in another corpus. You can find some [here](https://www.kaggle.com/datasets?sortBy=relevance&group=featured&search=corpus) to start you out."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
